{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers torch accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ks4oCqXuCxIu",
        "outputId": "3e0fea3b-b327-4951-f548-736373de3d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m867.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m917.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.28.9\n",
            "    Uninstalling nvidia-nccl-cu12-2.28.9:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.28.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cpu\n",
            "    Uninstalling torch-2.9.0+cpu:\n",
            "      Successfully uninstalled torch-2.9.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cpu requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cpu requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxwZG1WHEL3y",
        "outputId": "67be2d61-19bf-4802-deb2-66cb800931b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.3\n",
            "2.9.1+cu128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer**\n",
        "- a model that understands relationships between words (or tokens) in a sentence all at once, instead of reading them one-by-one like older models (RNN, LSTM).\n",
        "- It uses a mechanism called ***self-attention*** to figure out:\n",
        "  - which words are important\n",
        "  - how words relate to each other\n",
        "  - what the sentence actually means\n",
        "\n",
        "\n",
        "---\n",
        "A model alone can’t understand raw text. It needs:\n",
        "\n",
        "1. Preprocessing → converting your text into numbers the model understands\n",
        "2. The model itself → doing the actual prediction\n",
        "3. Postprocessing → turning the model’s prediction back into human-readable text\n",
        "\n",
        "The pipeline() function automatically puts all these parts together. This function do all the heavy lifting for you.\n",
        "\n",
        "**“Give me text → I’ll take care of everything → Here’s your answer.”**\n",
        "\n",
        "\n",
        "---\n",
        "**Sentiment analysis**\n",
        "- NLP task\n",
        "- looks at a piece of text and decides what emotion or opinion it contains.\n",
        "- It’s about figuring out whether a text is: Positive, Negative, Neutral.\n",
        "\n",
        "Some advanced models can detect more detailed emotions like anger, joy, sadness, etc."
      ],
      "metadata": {
        "id": "6oEvVxuYlBpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "374ee5e4767a4fe2aa6270bb9539e936",
            "d5ed59ff4c634ae7bfc359387cb98d95",
            "fe4dec50ea01484481c9f252a2e813ef",
            "6b8dfc3529d445a3b7f88cbd1f98dab3",
            "720fc1e9cd7949ba97c651941a57ca4c",
            "2753df84aef2469488f95865aba077c1",
            "5188fa688ade4bfab25cccb873fee355",
            "a4e8dfbd888c4e89b02a7f0ee0377233",
            "d75212a1b80648069b170c6f027e72d5",
            "eb22334fd8984f63850033dbe836564a",
            "accbb258a4bd40b5895b7a57a517a8af",
            "84ba6f5c3de44d13868977bd4b24ba40",
            "209a99520d2047a6b2f11b7c25a17fe7",
            "807e105a5cd540df9794b2cbd4dc1400",
            "ee5a8700a71b4aa58714da7968f8340c",
            "a43d0c148d27400aa9bed6e56eee478b",
            "20a7bf2631b041a29be58a8237f364fd",
            "4e5786e23d984b2d90d9143ada0e14a1",
            "248b87e9464b44eebfba81ef79dbe7be",
            "5dd3f98b95f04b53aad600222c0b7dc0",
            "96e1f7736c0849ca865d592b364c040b",
            "7b2a447c0d1749edace11cf3edf624c0",
            "7dd4862895a44d59b4fd7d61543e334e",
            "2ce54befe0c645cca1560af438d72125",
            "392c364bf7884760993f26fbe43cbfd8",
            "38e64644808542778eaa846abf9f3fc5",
            "0f96719a7bdc40c695e16c3279d0365b",
            "3c7eb8273eee46ae97425815cff57e62",
            "45bdad9e51a7476b85c73d2c84299f4f",
            "c69a0c638bad43dd8fb5e6d00c3a1c05",
            "928c2d103d9c4221a346ec029970d801",
            "99fe5f7bd91b41459f55562fc1046ce3",
            "584ac44f99b142e7bc45008eb2bc982c",
            "d380b22aaa374624841643e8095ab8d8",
            "c28be616bf07462bb07b53def4866e60",
            "ba4b8396c8dd418086dfbb7967b34b61",
            "d98fbc48f69f43cda258f6f104ddaf28",
            "4aafaae5df4a41919d70b3fea4420143",
            "7353ea667e5244a893e45c1c06ce4190",
            "03920715c798436fbea398278add1a6c",
            "f1297eebb80f4e2797e691c36ca95f7d",
            "25d0abe0171843c0ac647d25ffa4d9b6",
            "035f791d87f94325a6cfd4c1f6ce9367",
            "1a9f3b8d23d149b5be02ce3d9139319b"
          ]
        },
        "id": "qmcTIKRvgufp",
        "outputId": "937bbcbb-8a6d-4e40-8807-41c35b745b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "374ee5e4767a4fe2aa6270bb9539e936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ba6f5c3de44d13868977bd4b24ba40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dd4862895a44d59b4fd7d61543e334e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d380b22aaa374624841643e8095ab8d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        ")"
      ],
      "metadata": {
        "id": "B1uilOuchlLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8168c95-ba7c-43f2-8dc1-b1b4f5f76840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Zero-shot classification` is a type of text classification where the model can categorize text into labels it has never seen during training.\n",
        "\n",
        "we give the labels and based on the knowledge, this model has predicts probabilities for each label we gave is related to the input data we gave."
      ],
      "metadata": {
        "id": "bTkR2UsxmfUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ],
      "metadata": {
        "id": "wR8umyGri5ci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "01f7bb89cfb44827bacc16f9d3f50933",
            "bdf09890cd064abfba1cf3776aed1375",
            "06728b656a444305bd622a6c9652db2f",
            "32abb721ff33456ba80a49387eebdfa0",
            "f371f0826b4b40e7b7d0a215a42b6c69",
            "39db14b7ec5f488583565caa168961b3",
            "a2df067508174b40a9475fd0a62bfdeb",
            "8cb3df1a997f49589492dd20793733d2",
            "4f795271c0624ed7b6dd39d781defda6",
            "91503ebf1f074bcf8e872ecf65eb4051",
            "59ff14aebf574abb875010d828ac017b",
            "4e5d0bbdaa954382af71908574a764cb",
            "3908ce4eef74427ba452cf94fff43cd6",
            "f0bbb2fa286b4320b2d274d188584fea",
            "578b38c2391b4acbad01964e9ffaa3e2",
            "5c9a50c8ce054c7d8b4ab6c515fe0391",
            "006a3147c1544bcba6ebeb3abf9dc483",
            "267777dd91ad401aa1f0ae4353097850",
            "d2bce06cdb1b4162ab7e1496ea64611a",
            "8275c2123a4949c3855a66d6562228af",
            "b558623ca1634ce99635ed1162f0dc72",
            "9274ac457b1f4597a2a410af9c498641",
            "1eb2dce8a4c14423827eeb87274e6356",
            "202177068e084c2c80c710dadb564f58",
            "51da96b6facf48cf9ba58f9ad86719a3",
            "60f87da4ecd548a0a2d32f705fde6d6a",
            "bd529ffef3054b2da67cb91cc65489a3",
            "ea63501680e44f048a7ebd10fd51cde3",
            "06226314bfa84cd19094dcb24401f7d8",
            "66def997ee524822be2a4b1263699d06",
            "622a4bcc7a184df0872dd213a2087c87",
            "7ecf1452b7c9481b8cf6f0daac38fae2",
            "a6e787681d87481584c54c8b57921453",
            "da22b92ab53e40e18af8662a6c22ed2e",
            "c674d54065a54034aa5dfa017dda6b2c",
            "79c7629fe01b49a6a727f564871b2738",
            "73fb8116f2cf4dc3a6ef8a14d58f9a73",
            "1550bfd9c3524efca8989cc0165d88f2",
            "96be7bd0a85740ea841211d714bc247d",
            "599c7e6fe8624b358718457add5fa8a8",
            "ea0be3f0916e4944b5940dd37124d0f6",
            "7e46e63f259d4a12bdf761bd4e70eb2c",
            "dc5a0a60c95644c19737e6a167170007",
            "8420c18ee2304dd79dc9e8665fe3e3f9",
            "9d78db3a9ab045b69dd182e79c936f55",
            "00f86fb53b97465198e0feaba02a4eb9",
            "3e3f9b7ff4134dd7860567685679bc7a",
            "e67a15f1fc6c4a2683c03249519bd06e",
            "3ec16ac0f3ff4c798861fbb3db069c91",
            "381cfbf0d5e04fd7b4108e529beb5813",
            "ddf49939cae64e34a764bc42bea04c34",
            "175a784960334012b78c01d5828b0219",
            "bb1b26db5d7c4cd8a9e3c264726f59ae",
            "e5f5193e15ab4e828ba729b2186a31b0",
            "b76afb04660c4cc6a823af2bf3bc3e52",
            "4c06a1d0a14240e2b7daa4ffd68d5d88",
            "4967d5bee98b45f280f511a2da8f4b5f",
            "7f48532c223e4f278f03fb7917e1dd2f",
            "68ab734426794acea2a606b914609ac2",
            "8be64d26c243469f90e0018b3f512042",
            "a6a35e392fd14e9d88fa175728f10147",
            "af53e310c1f4400d85ad5f23e5e5c3c5",
            "34f14a08c5f54b45820b9fadf008e904",
            "51e33d880e8a4766addb0ad6bd46d9f3",
            "5da6df7d03d54137ae05f5adf11eb759",
            "126501513a0c4bcdaffe79d8f156f967"
          ]
        },
        "outputId": "260bb22e-604c-45d8-c5a4-5df1cb0bc455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01f7bb89cfb44827bacc16f9d3f50933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e5d0bbdaa954382af71908574a764cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eb2dce8a4c14423827eeb87274e6356"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da22b92ab53e40e18af8662a6c22ed2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d78db3a9ab045b69dd182e79c936f55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c06a1d0a14240e2b7daa4ffd68d5d88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445994257926941, 0.11197388917207718, 0.0434267558157444]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text generation** means using an AI model to automatically produce new text based on a prompt you give it."
      ],
      "metadata": {
        "id": "6JL80Hvxn46O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\",\n",
        "          max_length=15,\n",
        "          num_return_sequences=2,\n",
        "          )"
      ],
      "metadata": {
        "id": "S3sC2iobn5ev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626,
          "referenced_widgets": [
            "62123207e9b942cda5447b327c1e3077",
            "5cb325c8c4d5402fa199c50857147d95",
            "4374c1dae3f84e97bc53f5a2bc4e7b44",
            "47d041aa6cdc4187a02c4fcf6391dd72",
            "c9530c04634744f9b4d1827b0c993e7b",
            "0e6a044733494b6d8db07ea51a82a99b",
            "d8e50199d9724d0584bff18f014077f2",
            "5402e18f23e44b65ab6c54f5bda534ca",
            "9a9dbc3a458245e18a27e1df024b157f",
            "20445b1b50f14214aef6ff3f1554d949",
            "735d168675574e5c91a2d573639b4e33",
            "c9347cffa17748e3bcaff9b7239f766b",
            "0e9dc4cbb4b0410988b75454c7340201",
            "278258f8d9d04275b40e273df81d9c70",
            "54710e0e8c4d4804948380f38a7aa7a8",
            "e71d4268768e4f0c809d6ed722d657a2",
            "4e153895f0544c4a9e85984aa2043df2",
            "914c09c98a8d4dcda3c7309dc14303dc",
            "3ed8a519c29a4ad9874640cb0d04aa13",
            "76d1ea10418c43509f61022b2f283348",
            "d45014f45b9541148b6526f4e5744b16",
            "f893d9f4c7fb4b7d9e9f84466245956b",
            "5c3bf2945afa4eb4be2675cce166e47c",
            "f258c20982b24a5a96026ec3b18a015f",
            "7489d431fe37430688d725470ade12a3",
            "83ab5dbe14e2489a8b3790c0fae6bb23",
            "bf6e8ac548e84c8fa6069796b2f18e66",
            "be074b631e7b4bada952e06a1882d07a",
            "9de8b0caabfd4ec9a75ec068a5c74985",
            "d999b9c4659a4506a9db1d92d66a3a5f",
            "918a613e2f644997b664f7ec77095160",
            "a8bc26dcae6943baa17eb1d7b90b6118",
            "b83b3a9ff5f04ef5b15f59db293544e1",
            "e1da1e7ed91f478fad4a32555e148077",
            "d75da56745a84e54bf0e3e4f04364530",
            "38a1ca431e924db987e9c56f968bfe83",
            "7a77a1464434460e887c72ff72d2b7ba",
            "4f724aa9bc2e455bae6228a4cc626918",
            "278f678da2df495cb52ed2882b2009b1",
            "c2416dee995146e9bbd4b2e141ed89fc",
            "0781ff88a7ce4a14bfcfaaf0e2436d58",
            "a12bf602acd2446a9c7881364944b094",
            "4bbb8bfa3ade4cdba63caa86a8d61388",
            "ccd42fd6fbf045deab06e153f1c3a365",
            "311871b2e5464ce7a7f1bc8a4960c484",
            "2b2b72fc7e63480b8bce6f5fee00b393",
            "e8e05e9fce8a41c4a94945ac7d427bed",
            "640287961bae4b7f9a3719d3a9d78994",
            "a55290cf208d43fd901da9bffcd289c1",
            "76d374fe245b43568a8ddd0a2fa55230",
            "e6bfee02adf1409c902c02f6d58f1643",
            "528b8adda1c348388ca74643822a1585",
            "1f908d59021c4b9fa9ca1336e84db05f",
            "95524fed28ce4dc8a0fd300f608175cd",
            "6b97dc83a4eb43988b1306cad3743c38",
            "1b5bd36572364b0e96edfaff253ecbd8",
            "0df29f33635a447dbcb78d69eeb54645",
            "d55ad5ee3ce244aca63fbb1ebf24edb8",
            "57bb847474104ab3be83e533646e23b1",
            "278fc83ed35a4526bbca6cf5e0acc054",
            "c83c4a71708a4338900fa71341703b21",
            "26670495706d422a81fd56c4e3e041b7",
            "d1a52ab6cd594be59a12fd773b2738c8",
            "eb2bea20c4b24152b7636b02830b7af0",
            "5f4b7d6c0090483da045e0044329b7ab",
            "a02a948689884427bce542b5c3949ee6",
            "0fb7e9a50e114703b3596868bedd4625",
            "c50f9c6c065f4c848ea795c304c64692",
            "f5d056ace19c4f36b964ab7679aae955",
            "7d58a0784cb04d218119962572290808",
            "10555db3526b4ee8bea96f6f2e3d2c33",
            "8d95b766b2444574b53a7ecb0cae5448",
            "ebad190863eb43f4ba6bedeb2ba012b3",
            "949f622d75cf4be89fbbaef40d022e43",
            "032e62941ea043b8b9a0350c969d7ae8",
            "72fa2506ee81412e88e6e3c29a29f76d",
            "43480f3545f34507ac0e920c3d5c3c79"
          ]
        },
        "outputId": "8015a979-f7e9-4015-b268-af8b0e0c4257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62123207e9b942cda5447b327c1e3077"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9347cffa17748e3bcaff9b7239f766b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c3bf2945afa4eb4be2675cce166e47c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1da1e7ed91f478fad4a32555e148077"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "311871b2e5464ce7a7f1bc8a4960c484"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b5bd36572364b0e96edfaff253ecbd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fb7e9a50e114703b3596868bedd4625"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=15) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to use the command line to run the command line scripts in the real world. This course is not yet finished, but it will be very useful.\\n\\nPlease visit http://gist.github.com/tjr9mjxw/7b3fc5e5a20b1c4e8e9c8b5b9a7d/gist and subscribe to my mailing list. You can also follow me on Twitter at @tjr9mjxw, like my Facebook page, and subscribe to my RSS feed.\\n\\nIn the future, we will explore other techniques to use the command line in conjunction with Python, Ruby, and Objective-C.\\n\\nThe last post will discuss how to install Python, Ruby, and Objective-C, and you can also check out the upcoming course at http://gist.github.com/tjr9mjxw/6df6a1a3e1bb25e7f9d3cf0f9a/gist.'},\n",
              " {'generated_text': \"In this course, we will teach you how to create a simple, easy to understand but robust, and practical app for iOS.\\n\\nYou'll learn the basics of programming in the most basic fashion – using the concepts from C++ and Swift to create your app, then, in a very real sense, using the concepts from Android and iOS.\\n\\nWe'll also demonstrate how to create your own apps that are easy to use and intuitive to use as well as using the concepts from Android, iOS and other languages. We will also have a tutorial on how to create a simple Android app that is easy to use.\\n\\nWe'll take you through the full course on using the concepts, then we'll discuss how to run your app in the browser and then we'll show you how to make the app available on an app store.\\n\\nWe will also make sure you follow the same basic steps as any of the class of students. We will also teach you how to set up your app using the same framework from the start.\\n\\nWe also show you how to use the App Store's iOS and Android apps.\\n\\nFinally, we will show you how to install your app on a PC using the Apple App Store.\\n\\nBy the way, you can check out our full course on how to create\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "FyPgFAcJp1U9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710,
          "referenced_widgets": [
            "e5805edcc2f54667b6f42215adbad8ad",
            "89244370166a4f74a7c339bde272d094",
            "83780b0059314fddbab691ad12e2c3e9",
            "094f8bc2b6c140c9aa12a5b3d73bea28",
            "cfb6c92718d349adad0e7047872d524b",
            "0c6e84ef3868416b907f9fecd42404ab",
            "146ae15fc55249d4ac51ce1dc2d7b51a",
            "0bfeef6d05f543298990f73d18e4f021",
            "05d3b3fa1d0444ff8e0de57c095c1fef",
            "e3b1676870be444a85d50489543fadb6",
            "6534024721464566a840d3c968bec905",
            "211c7bcdd4a64855aa8c552724e5ed4e",
            "732316cc64844a8587f3bed31e3d52c0",
            "6ef904edb0334633928d2ab9e0368c28",
            "578cd3ec58354e2bad21d46fd7a6d5ea",
            "cd1c7e47bbbb40f2ba904a2bc99ed567",
            "41c55eebc64e4ff2a6d272c1dac07956",
            "10b978d19ff3450a89813c920552bf22",
            "7f77f1140aeb44ef8c7a09e58438a3a3",
            "badf136b0f064fa59f9bc033d5172044",
            "d4d0bc13365042d8a0707d167b1e1675",
            "660e8051837b47ee90f30c83c3e6dcc1",
            "dac537a0ce804566910ef6295af31bae",
            "598400e7586b4ac38e0ee7563a2da71a",
            "a174ec1ea84341e19d487830c20dcdfe",
            "edd914fe7fae47118c87145ef9f7f7ae",
            "ea31d12711e74990b7d59095aa1b4725",
            "58ebdc2fc08f487497f420465fd1c7b6",
            "4115f3051d554fc4a3d47a163aaf0311",
            "5f6247128f1b452097ae01669c6ad323",
            "fb756acbc4c74886822e90252c74305f",
            "ad22bf98c8a64f3b8eed43dd1318c981",
            "0ec5fd2858d94bcfbdc16811520f7e13",
            "f3ddd73ccc9440859615228d3fb26c56",
            "e2a545d7cb3d4292893d91c553f55c4b",
            "e380ea939c294990962258ca7b068b9a",
            "6cea8311aa7e4f249a7b0f8af7ef1c86",
            "94b2548fbb1244aaa41f74fdccb96209",
            "e783c8f07d224889a0e8d475f19ba210",
            "b5cf37d034f84b55a09b227b25f964e9",
            "b82f6d5aa62e4a6cbac6df665e66b8b0",
            "26ecdacd0b2b4ff2b73f8b6ce3f7e283",
            "f81fb8aa2f624e88b97acc8e60c6714b",
            "1e1ccbbca9374160869cc6f3f8b878b0",
            "c5508c2e35f04e72a9ea0b9f2e97cfa1",
            "866ab93ef805437da6a84a9a00dc57f4",
            "38e2d81593694725a7753f8b7fd4903c",
            "495ae501838e401ba21bbd2fc7302acd",
            "b5cd08c3b8e8454e97e7735bd304753a",
            "6aaf750d5fa0444a9843713d295f4f26",
            "bca626a581034bfc8703a723e565dcfa",
            "143732a6a2354f1db04a04cdaee16ece",
            "be7dffe7b92d4f7aae20d7fb45e44c77",
            "4aeff361a04c479badf1819b2f0f2c85",
            "c2c1f22a6b1c4798b2f435178f084393",
            "c0851879fb3b46e1b35716d2d1ec9436",
            "18957bdfeab04004bce43c6094fcfd21",
            "0fe01cd174cb4010bba49a031a11afda",
            "e9b89b936adc4531ac7b88e4a3c8e627",
            "09b77ee9506e404287f3760c161644d6",
            "08bcd81abe7a465bab020d22ed1fd248",
            "b22f4780f5d5491088262242f287692a",
            "7f6e02b44dae485dbd18215d03679315",
            "65bdea98f2c24e76bda0a9cf26a2c237",
            "4923adfeac334ffc908845434d0a8a94",
            "2654fdd2da2e4d608a243381634f95c1",
            "d567c6edb38144ab93efc45122daa3c9",
            "6e0cad67acdd47339d12136b6292c53f",
            "2417f810b1cb4fde874d69a9b4a16cf2",
            "e58d0cf7ccbb41a39628acc167f4029d",
            "b3fc68ee41c74aea88483c58b5c9a9eb",
            "1bceaca3212a4599bdedbe4d70a24f79",
            "3f453da1051a467fba0dc844a10362fa",
            "4f80a35fe1814fceb50f7098c15fc70f",
            "310d89f71fcd49068ddbc8987af1d175",
            "48acd60c03f54bffacc6eb016f30e8d4",
            "63a1c2167f6a462394999a44502e0a68",
            "c524f795528048148c7bab929732a460",
            "2cc06348598a46228b2799f074d22b67",
            "6d24bcaea7864794962ffe0c2bee21d4",
            "3a4651fdd6f84112b9e1bc764071a31a",
            "d70251392b214944ae5c9a165143356a",
            "82e49655ca634f95938fd63060234934",
            "22978bb91e2a4469999730dd0d9826ee",
            "6087a54b908441adb69aff6ca77c9155",
            "0f8d4a36ab424cdb9b9ae7dab530cd73",
            "510318a4d8e04016b00f63fb2622bd69",
            "d4eb5f4fc6614d44a35a15a70cf17357"
          ]
        },
        "outputId": "01a2f864-e601-4639-d61c-25e522a550c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5805edcc2f54667b6f42215adbad8ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211c7bcdd4a64855aa8c552724e5ed4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac537a0ce804566910ef6295af31bae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ddd73ccc9440859615228d3fb26c56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5508c2e35f04e72a9ea0b9f2e97cfa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0851879fb3b46e1b35716d2d1ec9436"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d567c6edb38144ab93efc45122daa3c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c524f795528048148c7bab929732a460"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to do the same.\\n\\nThe first and most important step is to learn how to read and write English. This will be your starting point for the rest of the course. After you have learned how to read and write English, we will teach you how to write documents in English.\\n\\nThe second step is to learn how to edit your English. We will teach you how to edit your English and how to edit your documents.\\n\\nThe third step is to learn how to edit your document in English. We will teach you how to edit your document in English and how to edit your document in English.\\n\\nThe fourth step is to learn how to edit your English. We will teach you how to edit your English and how to edit your document.\\n\\nThe fifth step is to learn how to edit your document in English. We will teach you how to edit your English and how to edit your document in English.\\n\\nThe sixth step is to learn how to edit your document. We will teach you how to edit your document in English and how to edit your document in English.\\n\\nThe seventh step is to learn how to edit your document in English. We will teach you how to edit your document in English and how to edit your document in English'},\n",
              " {'generated_text': 'In this course, we will teach you how to manage complex projects, from start to finish. We will help you learn how to create a project plan, how to define and prioritize tasks, and how to measure and manage the success of your project.\\n\\nAfter completing this course, you will be able to:\\n\\n• Understand the importance of project management and project planning\\n• Learn about different types of projects and the different tools and techniques that are used to manage them\\n• Learn how to create a project plan and define tasks for each task\\n• Learn how to implement and monitor the project\\n• Learn how to measure and assess the success of the project\\n• Learn how to communicate effectively with project stakeholders\\n• Learn how to manage risk and uncertainties associated with projects\\n• Learn how to manage projects from the start to the end\\n• Learn how to use project management software\\n• Learn how to use spreadsheets to keep track of project data\\n• Learn how to use project management software to create and manage schedules and budgets\\n• Learn how to use project management software to communicate with project stakeholders\\n• Learn how to use project management software to monitor project progress and evaluate the success of the project\\n\\nIn this course, you will learn how to manage complex projects using project management software. This course will teach you the necessary'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can choose models as well. [Models is Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation)"
      ],
      "metadata": {
        "id": "gmcEoz4XArLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "zW4RilUnAtpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7c74b6-8da9-47f7-dad0-76f3a50415cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to use the command line for basic tasks, and then we will teach you how to use the command line to create websites. If you have never used the command line before, then you will need to know how to do some basic things like navigating and editing files. If you are already familiar with the command line, then you will not need to learn any new commands.\\n\\nThis course was created to teach you how to use the command line to create websites. The command line is a type of computer program that is used to enter commands into a computer. In this course, you will learn how to use the command line to create websites, and you will also learn how to use the command line to create web applications.\\n\\nThis course is designed to teach you how to use the command line to create websites, and then to teach you how to use the command line to create web applications. If you have never used the command line before, then you will need to learn how to do some basic things like navigating and editing files. If you are already familiar with the command line, then you will not need to learn any new commands.\\n\\nThis course will teach you how to use the command line to create websites and web applications, and will also teach you how to use'},\n",
              " {'generated_text': \"In this course, we will teach you how to do just that. And that's what we will be doing. We will be teaching you how to do everything from basic calculations to advanced mathematical techniques to how to create and manipulate graphs and charts. We will be doing this all on a very basic level so that you will get the most out of our classes.\\n\\nAnd we will be the best math teachers out there. We will teach you how to do everything from basic multiplication to advanced calculus, and we will do it all in a way that is easy and fun. So if you are looking for a great math teacher to help you with your homework, look no further than our website.\\n\\nWe will be teaching you how to do everything from basic calculations to advanced calculus, and we will do it all in a way that is easy and fun. We have been teaching math for more than 20 years and we know that there are some very good math teachers out there that don't have the time or the patience to teach math to you. So we are here to help you out. We will be teaching you how to do everything from basic multiplication to advanced calculus, and we will do it all in a way that is easy and fun.\\n\\n## We will be teaching you how to do everything from basic\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='flax-community/Sinhala-gpt2')\n",
        "generator(\"මම\", max_length=50, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672,
          "referenced_widgets": [
            "34a7d4accf63493783c5f347783e9f0d",
            "d14f17c69f3f4e2db24f814cbc3f94f1",
            "f9e44a7e52354ee5aa920fe2b8d56b7c",
            "c13b7fefbb6f48a6ac51e6341ea2e237",
            "dd11b5da34af4275aebf2f47a2819a54",
            "fcaf09cc8f7747f891ea3ddb0835790b",
            "3d3f854fbfcf4ed4b2879f4600076bee",
            "852bbd13b40544a1a17e9269dd325014",
            "8cec2f18d669421f82901437b6572c5c",
            "55fc378ed49b4a2c8061219d1cd15dc9",
            "ea7b9c14ef73496a8fb14edaaeed78c8",
            "b400843e4360462ab57091a2cd424320",
            "b74246cc32af4d9bbd76e6aed03d98c5",
            "f9c07b74177d4fb8b1423b7ffcb03fd0",
            "dca8ef0c5a0447e3888387a7771644f2",
            "22184c51e56a42f88b7d72d69bb20c95",
            "2bc76ab26c124cc9852942a19c54fd07",
            "778e0b2a3a6c43f38ed4e60252b147bb",
            "b21a143653fe47a6bdbc0c837975b1f3",
            "7a1cb3d075394ab383a335143564349a",
            "10b2d2653491456bb28740c109dbcb27",
            "2317d2830a2543d6b9ba6d44ec2cb9be",
            "ee9e5227533f4850980790127f28f7ff",
            "a4c78aaee4034bb78565f8ea9ceacfcf",
            "9253fbe367e7460487ac75b0dd92b596",
            "3ffe08345db74c36a6d649b5283010b4",
            "3d99c9f70d2346e2914f18fae5f38dd6",
            "6268115474804eb18e0bb0e89384c2ff",
            "e17fe96e94da4783af2caae07c22fd60",
            "e1503a14c6134f1aa8642ab1e48ba626",
            "6965445110f3463ab45d52dcf140596b",
            "024bd808c3dc4aa6b428ed0cfd6d9224",
            "8507f27123ab44e0925935815f6c150d",
            "4e9c0e47d1a2499cae34c88f729b4dcb",
            "f0bff882221f4f33b666f3e9a91f8c42",
            "0d1d70fba7854bbbbe5205e2c6ccdf79",
            "dbec6a783bd84eb4ba05b378575c3e75",
            "bfc95f17efff4240bbea232f63ce3c47",
            "6d1c4f5f21474837988451a96d15bbab",
            "3e1186febfe64b528c9d6046d93340d1",
            "13c0724dd3f7410ea9b60ac0a136c1d8",
            "c6ed0029c7d94e6cb10a0a40c8a44e52",
            "d0361ee9e23240f68ac3cde4dcaefd91",
            "7b319a6d801146d9b16d45846d3589bc",
            "4af02acac23c4d89a49953dd4a89b88f",
            "f975f6c6c76142ceb423e86466d7ce05",
            "2dbe21c7195742f4bf53142236fa32d3",
            "fbb35eee30e64264abb06fa399bc4c10",
            "caadcd1a54424e39bcd512bbf324ef51",
            "f589286e96814a12b5fcab5004a921a7",
            "5ff589175b034b6794c0db923a3aa5ea",
            "19c9c1c7a31341049bfb3893ffd9a1c3",
            "df047b4757a94b718d8834c2dd504c1c",
            "f16b641972754135b7fc6ab4e6215aab",
            "2b8ddbf74ce046048f7a193d40b4b4a6",
            "7a2ae31cd202483b9ec1d03939c09d93",
            "debc33c1015e4ea4838b33c98cd0d8f8",
            "27f591448f9b453fbf4cb30a57de5530",
            "ccef4e3f8cdc42e4a4c1fa6cf9e36f13",
            "37bf9858358a4231b8812c9a050f0590",
            "bf7cca8dcceb4fc7b7141a5799ed60ea",
            "e70f5e0660204fc2adedf982eadfcc2f",
            "4c51b36d9af9449094a6a29080d014f0",
            "93d323079ca043d4aeb66210fd1c3bfa",
            "2204f55d978a46428400d5f4bbc6a4c4",
            "35b21f62d3bb4c1088f883b27e718d37",
            "cb2d09d1d81e4cd29999753927c2eb5f",
            "a2c3d5299d154c84a03e439a7912d370",
            "79af4087b19c43e19295362b193bfa4f",
            "bb1cb43f7ada400da46a7fb10c45fa91",
            "32bde41008084fa09b83658f2d000255",
            "25fdb4d47b074be9b726d4cebda0e415",
            "ce09c12a3c2c4bfd853977a693796fb3",
            "8e4384aea76140fb8f89dbb21fcedac0",
            "d93a0c252995483c84cfc697f5ec97f5",
            "58190338952a48c79f3647cfa51f2931",
            "b44542fe5391410b8b05b132d5dbfaf7",
            "b520a4f4728a469ebe8f9ebdc3f4515c",
            "ce0f60bb37174ab2a21e15f4658f4862",
            "f083afe36d6f43308bdb4fc8f21b8548",
            "810b17ccfe714ffca836909e2868feb7",
            "28430d0ea30f4d71a233f6cdebc334f8",
            "45cadae377aa4d30974eb2de7b98fc97",
            "fab1632ec9ba45d28561aa962fc98fd5",
            "95cfc097d02440c089e8d250e3a28a4c",
            "c0c8ff5aac554f05b5db0aed3c2f4890",
            "c487809ec7b6485c98dfa8e2dba484da",
            "118e894bc2a14cd2a5994744fe43a895",
            "c52e60e9aa8f4a52a72fb5e51f052835",
            "06d40ec97a29437eb0d9c919a3b77e7d",
            "27163a555dc8448e94dada564b80e8e2",
            "928487fdf0d0461099cf5b4247301abe",
            "0f89c8545f6742709c0d1ef5a4f68a47",
            "e11eb9a6571e431998bd9b5cb6e43459",
            "d6b126d519404fadae957d1a2d50e8dd",
            "3e637d091df543bb8f771c5a8374b3a5",
            "ad624c585cfb4a1fb5d00687858d1ba5",
            "0dc6bcc00a9641c18dfda92c30db2663",
            "e02d844b075e4ff4ab5fea5b176ab52b"
          ]
        },
        "id": "hDAu-YU3E3oY",
        "outputId": "fe81893d-cd51-4f88-efa3-d4f4676c2dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/868 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34a7d4accf63493783c5f347783e9f0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b400843e4360462ab57091a2cd424320"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee9e5227533f4850980790127f28f7ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e9c0e47d1a2499cae34c88f729b4dcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4af02acac23c4d89a49953dd4a89b88f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a2ae31cd202483b9ec1d03939c09d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb2d09d1d81e4cd29999753927c2eb5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b520a4f4728a469ebe8f9ebdc3f4515c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c52e60e9aa8f4a52a72fb5e51f052835"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'මම කියන්නේ මේ කෑම වලින් නිසි පල ලැබුනම මිනිසුන්ට වැඩිපුර ප්\\u200dරමාණයෙන් කන්න පුළුවන් කියල. විවිධ දේ ගැන නොයෙක් ආකාරයේ ලිපි ලියන්න විවිධ අත්හදාබැලීම් කරන්න වෙනවා දිගින් දිගටම. නමුත් අවසානයේ වෙන්නේ තමන්ගේ ස්ථානය සෙවීම සඳහා කෙනෙක් කැඳවීම. මෙය සාමාන්\\u200dය දෙයක්. වෙනත් විදිහකින් කිව්වොත් මේ කෑම වලට වැඩිපුර යොමු වීමෙන් මිනිසුන්ට වැඩිපුර අහිතකර බලපෑම් ඇති වෙනවා. කෑම කි'},\n",
              " {'generated_text': 'මම නම් කියන්නේ මේ දේ නැති කරන්න ඕනේ. දිගටම කියෙව්වා කියලා වැඩක් නැහැ. කියන්න ඕනේ ටික විතරක් කිව්වා. හැමෝම එක්ක මං සුහදව ඉන්නවා. මං මේක පෝස්ට් එකට දැම්මාම ගොඩක් අය මාව පිළිගන්නේ නැහැ. මාතලන් 26, 2013 10:58 මේකේ තියෙන්නේ කියන්න ඕන දේ නෙමෙයිනේ. නිකන් නිස්කාරනේ ගොන් හරක් වාගේ එහෙට මෙහෙට පැන පැන දානවා. මෙයාලා කියන ඒවා විවේචනය කරන්න දෙයක් නෑ. වෙන මොකා කිව්වත්. නිකන් ඉන්නේ නැතුව. ඒ වගේම වි'},\n",
              " {'generated_text': \"මමාලි බීබීසී යේ කැපිපෙනෙන චරිතයක් ඇය නිරූපණය කරන්නේ අලුතින් කැමරාවට පිවිසෙන නව සැමියාගේ චරිතය කැමරාවට හසුකර ගැනීමටයි. 'මමාලි' මෙම චිත්\\u200dරපටයේ ප්\\u200dරධාන චරිතයක් නිරූපණය කරන අතර 'මීරා 'ලෙස තම ජීවිතයේ වැදගත් අවස්ථාවක් නිරූපණය කරන්නීය. ඇය මෙහි අතිශය ජනප්\\u200dරිය වාර්තා චිත්\\u200dරපටයක් වන 'හිරු'සමඟ සම්බන්ධ වෙයි. 'සුරූපී රූමතියන් නිරූපිකාවන්ගේ ක්\\u200dරීඩාවක්' බොලිවුඩ් සිනමාවේ ප්\\u200dරථම වරට නිරූපණය වෙයි. සිනමා තාරකාවන\"},\n",
              " {'generated_text': 'මම කියන්නේ නැහැ. හැබැයි මේක විශේෂයෙන් සඳහන් කරන්න ඕනේ. මාත් පොඩි කාලේ මේ වගේ සෙල්ලම් කිරීම හැමදාම කලා. අද ඒ වගේම ඉස්සර මේ වගේ සෙල්ලම් කරන්නේ නැහැ. අදටත් මේ වගේ සෙල්ලම් කිරීම විශේෂයෙන් සඳහන් කරන්න ඕනේ. මේ වගේ සෙල්ලම් කරන එකේ තියෙන විශේෂත්වය තමයි ඒ සෙල්ලම් බඩු පාවිච්චි කරන්නේ අනිත් අයගේ සෙල්ලම් බඩු වලින් නොවීම. ඒ වගේම තමයි අනිත් සෙල්ලම් බඩු වලින් කෙරෙන්නේ අපේ සෙල්ලම් බඩුවේ ඉන්න ළමයාගේ සෙල්ලම් බඩුවට'},\n",
              " {'generated_text': 'මම තුමාගෙන්ම අහන්න ඕනේ වෙන දෙයක් උන් දෙන්නා මාරු උනාද නැද්ද කියලා විතරක් : පාටලී චම්පික ඇමැතිතුමාට සුභ උපන්දිනයක් වේවා! : 23, 2010 11:25 : 24, 2010 12:07! : 23, 2010 10:45 : 23, 2010 10:58 තැන්ක්යූ සුභ උපන්දිනයක් වේවා : 23, 2010 11:35 : 23, 2010 11:51 : 23, 2010 11:55 : 23, 2010 11:55 : 23, 2010 11:57 : 23, 2010 12:03 සුභ උපන්දිනයක් වේවා : 23, 2010 11:58 : 23, 2010 12:01 : 23, 2010 12:06 සුභ උපන්දිනයක් වේවා : 23, 2010 12:16 : 23, 2010 12:22 : 23, 2010 12:23 සුභ උපන්දිනයක් වේවා :'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mask filling** means fill in the blanks (`<mask>`-> mask token) of a given text.\n",
        "- `top_k` - number of possibilities (answers) you want."
      ],
      "metadata": {
        "id": "RroXBocaGJsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "654f14464c464da5a2dc82c4f18fc950",
            "f2fc2ab190ac4065b3dfef4b6458036d",
            "8f0c5d6fb3424356b2cea2e8988eb058",
            "fbc7685db5f74fecaa3b93b9ae5bd6e5",
            "f86595322a67406083c4b9aa307cf7de",
            "437e1d685f544d3c8ab62d7db3b066f5",
            "adc78e4dbdf84a46abf3848e4232883a",
            "c9c16bb78d8a4751ae351fe58e46062c",
            "3707bbf2120c4c298d1f7d17e96a4fa4",
            "3b40319a1ad745928bdae13f93bf839c",
            "772a7cdb0dcd4b8c93375a97e059e77c",
            "0b7ac3183b0e4b2d8f8afc767384d780",
            "59c53ad472fc4dfe9e70a00d9808641d",
            "189447a0d831486786dc348f6aca6365",
            "b16f78bb0d5d494aa028b05cc7915fdf",
            "7f250cdfb3f24e3fbf72cb5c72e336ce",
            "5fc91e4b0b304b2a9580ce7dbbd9b15f",
            "45cce8bac88e49d998959e02ef06003f",
            "36a050468fb64e65a3fd944d53831d06",
            "0d56af73ee654d6ab18d22b2199bbc76",
            "6a75b5f730c04b198075485abcd3f051",
            "03232966e1e74fdfb04d128956bfed8b",
            "d3f6c49dd7414ab8b9e92806c2172ccc",
            "3258a35e2d23427d8a5deda06472bfcb",
            "a53a6f9f5a9845d19794a816314ceaa6",
            "157311cc3f42452ea72b1e6745a27282",
            "60be0d4560f948ed8f6bbd1b10b73495",
            "442552c8ea874acfb5782e6dc7b7d02d",
            "b0edf7a60ca94869849546951f0a1918",
            "54d8ad07c2924c07a51fa46512de0c1e",
            "82725aeb1c87407cb2ab56ce7a2c4e4f",
            "7df8883aec85414c9048208624b29f6b",
            "1cb6819ad6ee4b83a950ab38e5669f5f",
            "d1c593e23e424b1daa75658b0d87da93",
            "4d9c5495645a4f48bb0e7b25049ec498",
            "6115a21d05394ad483eaa7b9728e0bd2",
            "90520ef790d0485da2c672e43d02a5eb",
            "74f8dd5bdf444a4ba289c5065764c56f",
            "5d020914ea9c4d42a184dd30a50e4dbb",
            "ba8bf5079dab457fa0b50337c5675fa4",
            "ef550468e28547e098ee65676425f0f1",
            "0118491f4ab1433da026c7ef881d7cfd",
            "9a5586475fe34f80b6a4983b638cefe7",
            "7a47ad6e6dca4bf780d3a0db83cb28fe",
            "bf72444c203a48a9a777a4560f21a052",
            "b0f224031b2e42b7a9b76dcb4257da0a",
            "635e52d9d56542bfa11bec04658e1af6",
            "d92916b7428a4991bd582c9757954afa",
            "3971c91b353c4243b326beb65cdaa1f7",
            "228b2dfe03764eda9375f73add599bfb",
            "449fd7acdab6442bb4c0e7b7a6da8e5a",
            "a903e305b0524ed8b33a44ea4099589d",
            "6308a9cb299e4c61bfd673f10e7ca0bd",
            "b01601bfed274420938a76b700c85206",
            "2a964a07343a46e58c30f83e64e4dd18",
            "69eeda7c41b846e69d38d04ed29b6269",
            "c0dbefb8f2c441bc8d1025f8e4a47dd9",
            "df103f9dda8e4c47a0cf7dedfd7607cb",
            "a3ee4c12873e4e428c15b85185f79a5a",
            "4b44f498352848c884d699bce98e595e",
            "a375e14978b3430196177b1ef4eee723",
            "31631c1316894bb38d24dcec6e3fb948",
            "f719ac355eef411fb01d2b7689cf5f88",
            "d8e9618bccce4a77a6d3a926b7e7c894",
            "b2dfa52ab87d42a5bbfeb0cd1c2e42d3",
            "6517c225fc3340319f0d7f31be7d81a0"
          ]
        },
        "id": "GJkRobL7GCeG",
        "outputId": "19e0d8ef-4e0a-48c4-908e-d07652d98512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "654f14464c464da5a2dc82c4f18fc950"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b7ac3183b0e4b2d8f8afc767384d780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3f6c49dd7414ab8b9e92806c2172ccc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1c593e23e424b1daa75658b0d87da93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf72444c203a48a9a777a4560f21a052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69eeda7c41b846e69d38d04ed29b6269"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.19619743525981903,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052726551890373,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named entity recognition (NER)** is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations.\n",
        "- `grouped_entities=True` - tells the pipeline to regroup together the parts of the sentence that correspond to the same entity. So Hugging Face is considered as one organizatiton eventhough they are 2 words -> 2 orgs."
      ],
      "metadata": {
        "id": "vi6j57tDHfOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564,
          "referenced_widgets": [
            "267550267f65490784f67e1d65627d7c",
            "2678a38b49b040ffac2a0d4a7b2d4460",
            "d59ed21e373542d9ba8dfca4ca718165",
            "3722cc160abc42ee83d65ffb0dd0d2a7",
            "218f85da54974dfdb4244ed666079624",
            "d6644a0922da472b8448c7e4bb434c5b",
            "e06fa75fe688494c8bbd01273a25f4c2",
            "78a23840236e4ddb8ea6b4c0929c290f",
            "e2e9357684664229b213f25e93569e84",
            "64627cd26f6b44c082fb2219969b1181",
            "4bd55e668141491cbbfe80f0d4b2d675",
            "710ae2fa76184c44a7e8c19e0649e7f3",
            "89bf3fa507d24687973aebc9b83af45b",
            "e3178d76c97846c686e77ac138f9ffe2",
            "5b3ec13d591346f2a53471d168251566",
            "cf946104104e4dfc93d4e8c2dae8910e",
            "19c7bce3da3a4cf3b93eca0e0fa6b5b5",
            "a556c14857134cfb8de5327fe5aa749d",
            "8061c4fa33e4497090453a7c7797da4f",
            "b0775048ecb8467ca682816f58ee8773",
            "c5e9024cc71f4f798b823aec4341437b",
            "c260dc1c669c4f98adb1b2b36114344a",
            "e377fc801d74439e891645689979dfec",
            "cdd6cfc793b946fd92ae8a8a362a815c",
            "071e168291f54bbc98ec54169485e4cb",
            "a406c3d7077b43099127f8db47b3754d",
            "4740c81d43e44a55bf2f5390f7e98f5c",
            "4a04b29005f64335b6b4eaf2defb659c",
            "8ff38536362948139372868791f2b5d7",
            "fadc08a228214ef1843a130aad4f3818",
            "c021061927204fd19edf116635a3bcad",
            "e0311ca9c7d649c6b7eafb981208c836",
            "d3c4711d9ebb4e23a11f149418bac0da",
            "6f7742879da04860ab26405426fc2a8d",
            "98202f014a5b441787d1cda9c245e899",
            "35870aca17fc4ff0970223730d641577",
            "9994d549b1a04792ba079211976d0446",
            "4210454dd81d4365b14756eb34076c40",
            "1887eb63ac8b44bf8b05d3e51c1c03ca",
            "67715e706c0348faac8e2609268918ce",
            "33d3ad880ca749e7abedc24c96cfdf37",
            "df9f05ce45aa461693564280075e3d47",
            "cda868c535f3497c86e92b65f6044874",
            "e972c7431e494e90a06a9be643d703da"
          ]
        },
        "id": "goVYCh9dHAZj",
        "outputId": "1fe6d4c9-3759-4289-e1c2-d9ac4c096c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "267550267f65490784f67e1d65627d7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "710ae2fa76184c44a7e8c19e0649e7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e377fc801d74439e891645689979dfec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f7742879da04860ab26405426fc2a8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.9981694),\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.9796019),\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.9932106),\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-of-speech (POS)** refers to the role a word plays in a sentence.\n",
        "It tells us what kind of word it is.\n",
        "\n",
        "| Part of Speech  | What it does                   | Example              |\n",
        "| --------------- | ------------------------------ | -------------------- |\n",
        "| **Noun**        | Name of a person, place, thing | cat, Sri Lanka, book |\n",
        "| **Verb**        | Action or state                | run, eat, is         |\n",
        "| **Adjective**   | Describes a noun               | big, happy           |\n",
        "| **Adverb**      | Describes a verb/adjective     | quickly, very        |\n",
        "| **Pronoun**     | Replaces a noun                | he, she, it          |\n",
        "| **Preposition** | Shows relationship             | in, on, under        |\n",
        "| **Conjunction** | Joins words/sentences          | and, but             |\n",
        "| **Determiner**  | Limits a noun                  | the, a, this         |\n",
        "\n",
        "In the model used below, these maybe slightly different. 👉 https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos"
      ],
      "metadata": {
        "id": "fmBAoqs4cvWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"token-classification\", model=\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
        "pipe(\"My name is Wolfgang and I live in Berlin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9024d908972459b83b57651c324d16a",
            "c93fd6ec3cc548c984372fa066480662",
            "a9587176295e4ee89304fdcd4f360af5",
            "afc7a718246840e6a760a10d40744382",
            "a82ddc1537c6450eaef0dcb96e8a8e0a",
            "55a6210ee435410dbe806ee57f2edc1e",
            "82dac23616fb4426b459ca2fb6db8882",
            "97efc78f07564b169434bf853c58a588",
            "332209e72ea94fb4a1f37a0fb8a03cab",
            "b866cb6d5e404128abc497b15a183838",
            "ebaed5359e31408bac88c051a021029a",
            "c01ebf9a8730417d84d9debb45f91b6c",
            "0abe736c8d634128bc77394992cfc060",
            "c80be0edfa4645b7a8d3f77aa14f4347",
            "7d3f0e0ae4f64580a575e5b52993139c",
            "2eba3afcd5d2428083c6738021e1e855",
            "8a89832cc5e5466584979cee2d7df980",
            "dd1672b7721f47e68539e18c6ea9a72f",
            "b7d97a71670c459d8ea6d41dfad2fd11",
            "aa72fbd01bf84f9596fbdc31614e8b84",
            "ab99b26f70ac4a0b956fc8652fe52e15",
            "99c6c505eb914dc6aab0e6e1e79a8f87",
            "7b70700dfb194717a6d0a92c18787308",
            "d3356a92aae94eb89fde28d2d619df79",
            "0cb8285d5dc04f9e87b8c779f97bf987",
            "1b7d6f33d7b74e5c8c6f76604d052c04",
            "d9c255a79056428e9c39acbba4aa2c25",
            "dfd3a3b0dacc406f95618cdc64a9faf5",
            "8e14b4a14afd49fc9a69aaad175651b8",
            "6da3feb373bc488fb959522127d2ba71",
            "6458c1dc64c2446084b86a4a310da000",
            "bfc3a1496eb4440c9e00af2c67cb93e2",
            "4dc8a73b9f8f44c5b3e06972b29cb3ff",
            "3d533bb488c749a3a305b401255f35f3",
            "cce2843924b84097aecb7cbf574d30c8",
            "d9a933c29f4b421fb5307c529ce21674",
            "d6eeb9b01df3412ca8a4588e44352a87",
            "a9d917325fe742c497c27faf9e074d48",
            "5945fdd608a24136be2e18fbc8fe68c4",
            "fb72b3999a5a498c9defd6568bbb44d7",
            "4f4ecf2cf1df43e2a1c754911bb32888",
            "23ba0c8e171d4c06b984655485ff54a4",
            "51fe99c113d943d9bfeb0c2419643788",
            "b0a38ae9c65b4a4982bdd999cc87662f",
            "df5b287568fb4684aabcf812e00358cf",
            "32c1afe7d0c9412485735ddfc8bcb552",
            "55100676b3814698949048c4af9587ea",
            "5a0201f8bc7247a586870ffab17728a4",
            "422c739efca9490ea8975e2cded0c572",
            "55742b3e77b3468ab64f5da59866ab30",
            "96cd290a6d2046c0abfabf7010836219",
            "b52afb4a020c43109b314c1cdba7978d",
            "3b656cc457db46eb802cd6c574923e84",
            "328b1dc8432048339a0c5508c749dce3",
            "c99a74db52b64acb9c18ea9a2f9ad81e"
          ]
        },
        "id": "pynY8FeubZI4",
        "outputId": "2c160ab0-c6c7-47d6-848a-e37116ec9ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9024d908972459b83b57651c324d16a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01ebf9a8730417d84d9debb45f91b6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b70700dfb194717a6d0a92c18787308"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d533bb488c749a3a305b401255f35f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df5b287568fb4684aabcf812e00358cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'PRON',\n",
              "  'score': np.float32(0.9994399),\n",
              "  'index': 1,\n",
              "  'word': 'my',\n",
              "  'start': 0,\n",
              "  'end': 2},\n",
              " {'entity': 'NOUN',\n",
              "  'score': np.float32(0.9972766),\n",
              "  'index': 2,\n",
              "  'word': 'name',\n",
              "  'start': 3,\n",
              "  'end': 7},\n",
              " {'entity': 'AUX',\n",
              "  'score': np.float32(0.9953056),\n",
              "  'index': 3,\n",
              "  'word': 'is',\n",
              "  'start': 8,\n",
              "  'end': 10},\n",
              " {'entity': 'PROPN',\n",
              "  'score': np.float32(0.99878293),\n",
              "  'index': 4,\n",
              "  'word': 'wolfgang',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity': 'CCONJ',\n",
              "  'score': np.float32(0.9991617),\n",
              "  'index': 5,\n",
              "  'word': 'and',\n",
              "  'start': 20,\n",
              "  'end': 23},\n",
              " {'entity': 'PRON',\n",
              "  'score': np.float32(0.9995028),\n",
              "  'index': 6,\n",
              "  'word': 'i',\n",
              "  'start': 24,\n",
              "  'end': 25},\n",
              " {'entity': 'VERB',\n",
              "  'score': np.float32(0.99733955),\n",
              "  'index': 7,\n",
              "  'word': 'live',\n",
              "  'start': 26,\n",
              "  'end': 30},\n",
              " {'entity': 'ADP',\n",
              "  'score': np.float32(0.99938774),\n",
              "  'index': 8,\n",
              "  'word': 'in',\n",
              "  'start': 31,\n",
              "  'end': 33},\n",
              " {'entity': 'PROPN',\n",
              "  'score': np.float32(0.998801),\n",
              "  'index': 9,\n",
              "  'word': 'berlin',\n",
              "  'start': 34,\n",
              "  'end': 40}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "2e65f55177334e6e99dabe780bb03cb2",
            "bf6110569be34883ba052a186ac7b591",
            "c3e2965be350478f9c21529307821266",
            "121c177754264b39acee7bfcefdc9617",
            "c3683c1ca8484fde87864a6a355ba718",
            "0db89a93e2b74a21a6dc456134aceead",
            "4edb19a52e0a49d09ef50f6f4823fd8c",
            "8ed4fd2e2061455f904c48a97715d689",
            "aaa6e07b24f24a8b861a695a1da5b8f5",
            "37bb2b899b8248e8893a77bd556a8b57",
            "e56a03b244ad494d8b422e35afe412d2",
            "d17330c2d7384e7194bc7e258d11d201",
            "aa55ac89478b475984ab2e81bc745b91",
            "e44368920924458f9e7a3a75d5fcb84b",
            "cf53fc3f52c84e7591351151ef279d0b",
            "9074110c32734c608b1b28c8fc309924",
            "418ac4415af5482699107ab300df77f2",
            "ae2a1f1ea2894bcb84fc65622c2c6b99",
            "63ba2d2071574063a67dc771675f3f0a",
            "d49d5ac46901472aa27976a88fe4aed3",
            "d29b92c90b1a4fa8b5301948bde062a1",
            "964a00bc7f7747bcbae503a4c3e395ba",
            "1bed85a84cd24f43b2ddbe0ea1116ea1",
            "7b74b4f9d0874fd2b5c466a4b634dba0",
            "8f4f013356344e07b273821a4f1cfb7c",
            "3c54fe6eaa134034884e45461e5f3bf8",
            "b4a25eff36ab4b3483945049371102a4",
            "2a6a38be4ea9442fbd416038dbbc9f1c",
            "8094489acdd545208f24924a85fbb1d5",
            "3d5395ef97bb433eb203e21cebb91ef8",
            "efaa955153b84d7199d01ec4da98aafe",
            "377954aeb5f548be9026e7d37ca96875",
            "c0b1b51e81cf485282cd7aa58fbae140",
            "416c9ad60b844c3b992b17c2eed60b6c",
            "99ca382bfeea46abb6a5e75c8a7bb44f",
            "65a0024cf4f342eaabe5ae99f9d2f8bb",
            "e4bd08a03bc448e8a6d08894e4fca035",
            "1c2e74edc2ec4602b012938d05433f32",
            "52791535309b4caa990bd743457d7e21",
            "d4068e97d3ca477aa0f9142a3cb9eee9",
            "998a0772b72c4fd7a7397d93fc48552e",
            "56a9c833999f4639a8a23499973e7309",
            "09c94e1ac4df4134809fd4ad3148117e",
            "968f4db83aeb41029517f0dadd173290",
            "a77f427583e34a11aa4acde587e047fe",
            "3d39336d032f4e3bb6d1d8cfaa46049c",
            "bb96603e10fe411b87ddd4d2495d96d7",
            "b192827ab93d48a2acac936ae1075e6b",
            "bb13b0ab261e4710a20a36ddd84535a9",
            "5ae1fc5ddbd44b28849f746bf4d1dbeb",
            "2aa92b0005a84b119fbd1a9c54ccb888",
            "27bf8dfe42d5472bab7f33ed3573d4f9",
            "65df7fae6b554a08b1a03070d9c3d8b9",
            "f06c450306f94cf685df066e8c84b7a8",
            "23ad1b64a6e943fd9813211784ab7903"
          ]
        },
        "id": "J2lnxpXRdzlm",
        "outputId": "b217bf1a-ba39-4f54-ee4a-b64f8e7f296c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e65f55177334e6e99dabe780bb03cb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17330c2d7384e7194bc7e258d11d201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bed85a84cd24f43b2ddbe0ea1116ea1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "416c9ad60b844c3b992b17c2eed60b6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a77f427583e34a11aa4acde587e047fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.6949769854545593, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "5436610e62af4e13813cdafe936f91a2",
            "96509d466fdc4bf19afdc547b901e118",
            "8615f6ca892c4146996cd0bd41df56df",
            "fbe40f674d424ce49e28bf362e7dfc40",
            "699fac6224b440bb82943e063a65660d",
            "3196db79bcb8442d86bdba84943606bc",
            "e600521cfd78487db204b1191eb94338",
            "fb6ba0a3422f4a40901f1d3234a2dea8",
            "80e3960b8a0c4b449663971e5b57e7e6",
            "48bef30dce6f4687a02cb0820dc94016",
            "c2e57bb2d2294856b58746fb6c1dc7cb",
            "92ad24b0f52d4e8089da78ba5b779c1b",
            "8610b411812f4aa995c7c99b9c907ceb",
            "584effb098384dda86571983dcef650b",
            "626aaa55b31a404eb4bb7ff244c50b46",
            "9f994adec4234a198ecda635dd4e1470",
            "497ff704b2a242559b1a5044021a4826",
            "a227ba1c54be4c0abe176fe0a847ea00",
            "0ca15a427df94263ba026eef738831e2",
            "6aea46d9f5fd41498facb7d8c41d4023",
            "a096f8b55d0c4a10a4307dfa125f62f1",
            "9a31517ddc164639a57dbfef9ce0a5b1",
            "d7409694e90b4634a7c5876b3943314e",
            "ab0867fa220e41ba9d4a56941c8d07da",
            "8f33c4325f594e24b92df7b2eea65668",
            "d28071054d4b4fc9b17452f3cacdc605",
            "ced287258f754c668f8969dd5066c7bc",
            "c24ca86f38554e489abbbc2dd6dd7c46",
            "32681451e8234d42af6084622d0c8398",
            "674b5551b1424e15a5ec1740bef122d5",
            "7e50bf4907104632b3053835ea5bf11a",
            "6d092ea870024f65a226bc51a7c44178",
            "7cfacf03fc7c436f8a90e6bd4e542d2b",
            "b2b5a96a297c4727bc8b44028a0919f1",
            "c4335a2808e0437fb9db3dbb98168f23",
            "0d61651a22fe4340a39a850dd477c213",
            "7bec3658c59b48549c2bffe9f9512e41",
            "a1e6daea3c094f7ebeb9ed9eedb02331",
            "69f54247b35140ab907dee3280c390cc",
            "9038991bd63242e896f27dcb732a3064",
            "08977271e3444873b1779ea3237fe407",
            "27fe7b35d5994ad2b8ac4beed3a4816b",
            "54c81c0c1025411480843045f666f835",
            "e977847701394360a7a14df729e7ae4d",
            "3bc9c429bcb947088fb1c8255c797791",
            "faf8afe0586b4533a141891080cfb172",
            "457e4e4c59ca4018ba20272a722cae34",
            "0b6bd513ade54d3b83c42d63939118c6",
            "9d94d80e819b4f3dae6374e310756448",
            "3ca983e85fb448c0bb5430f868c7e256",
            "18856c4bc7d44b37b4875929648f260e",
            "ce02d93bc25b4a28ae3c0887f9b35113",
            "41258d68ca2c4ec29f27047be81b10b8",
            "c089d38e4f474e7e9ed59268933e46b6",
            "d1017bcbfb124ef08a8a0e3272cf39b4",
            "aa55cbd342144cc2b44bca0f42dd3096",
            "ebfc979c826b487d9bd8abeb3652c681",
            "8ae6a5e8f1794d27a01b540db9762647",
            "b5baf4a4a33c4165b80b2fcb7cad4b2d",
            "437d3c14acba4886aff92f3b51f4b384",
            "583c642d95974cb6b9e3bc3da07fb1bf",
            "0f1c84b34324471e8080cd4e045b9869",
            "0a2577cc5db44e78a9bd0151e55c8408",
            "59bc8b794bf842b6ac5ea0586edd8b74",
            "d95893476d0a406ebb455da77e3ae355",
            "a1bfd124e499430eace274a0c36402b2"
          ]
        },
        "id": "uGYoGQhrd9FX",
        "outputId": "2983fd43-b973-4838-c96a-145f8a849362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5436610e62af4e13813cdafe936f91a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92ad24b0f52d4e8089da78ba5b779c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7409694e90b4634a7c5876b3943314e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2b5a96a297c4727bc8b44028a0919f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bc9c429bcb947088fb1c8255c797791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa55cbd342144cc2b44bca0f42dd3096"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "0835d2bfe0514442a29d4fd72f86ece0",
            "bc2699e2636f4f75a43d850b6037a00f",
            "e53034433d484d80b3cd217088100adb",
            "9b270d56944e4529b0cd1e280cdbe6b6",
            "ebf3dd646cbc424c8e79fd269afd9b19",
            "42bad7b490554b719166205fc0efabb1",
            "444b08c39e944cd99aae995dfb4804da",
            "a48c2ecf78bb49798c7b8ad30f58b909",
            "d9df0940a0814952b87e8b55ba6616d0",
            "b9d7fe71b2194cb38ac874ee6daa2ee7",
            "bf3ed34798cb433c9b6aefc21c1d4687",
            "564d51582a88411f9225fdba6c991cd2",
            "e785a9ab35154fbebf0f5c1142a74462",
            "c8d0091b4dab4ae991bc09949d4fbb26",
            "35c02fbfa45e4daf9c39456589720986",
            "f898bdb9633547dca6c8c0bf10803309",
            "020090be75b14b73bb2cc1171a13dabc",
            "acbff2b6fb384a71a887cd46557c3840",
            "d280c3b90e92449ab376532f300bee10",
            "0fe636b12c7d42be9524f92be695456b",
            "249e71b3a8b94b3180c500d33563aa5e",
            "e7e1c760e84149d8b76da14a2d94df5c",
            "1eef7b00c71e4a5cbfd58da987011cda",
            "dd40d6573d6449dab7d9d5798a93a1a0",
            "94ef29588b174824ab026742beb77aca",
            "f35e62f716cf48f98f43e34aca2f2594",
            "636f0f8776d5413fb1b0dab846ed2ce4",
            "f6f0d55dd8994267ba4bf9dc7effe568",
            "1cd03284f3634667ac152d6b438fe946",
            "a3c70d0611be418dabe6541ca092e5db",
            "da764cd005e540eea1c598c2106c54c9",
            "c2df9d2031f34fec8c036051f4cc443f",
            "6992ac907c1e4d1ea36e6013e9375eee",
            "1a6c8dc5c476458cbacf97c9d0b61745",
            "dbd53aa0f64b42bc8d172b98f65e0ebb",
            "68cb7d1cc97f427289c464a07fd7ffcf",
            "32a5fd6711e34c478a4a28d1ed3867a5",
            "846c4570114b4b9cb54e27282d30d3ea",
            "b93c197a5db34040a806b7be018db93e",
            "63bfe7c1bb634a9c8253f628a78f7bde",
            "0e443f54318c41a9a8d70ee7cb0ca6ad",
            "bcc22651cf9b44368aae364646f02ab5",
            "200352650c45427db1ed279807749c5b",
            "fef3695ff4ed46edb85c89f3c26804c9",
            "8b64094b12ca4b6c8fbc579f09eefa55",
            "1e10c1b35bc34efc92327e67e3d3eba0",
            "bc797ae7b86b42c6a4865d87f060055a",
            "5932cb7d449349a780f6bf3ff06c583f",
            "23e74534f86f41d7a9c23046a471c831",
            "302eea81e4f940e5b5059a7521ac360d",
            "13b815f414994011a1eb8c1c7ca3b310",
            "7e7d91abf9994d0490db9baa1fe6b861",
            "984fac9bd4c94068b66ea338cfbd25f0",
            "620f279b9daa4f4885011c36a9c8045e",
            "9f3391704c134e6990d6b46d3b6f5f3e",
            "d5f80c69b25a4cb08af2f0498dbd5753",
            "689986395dfa4b82825de4da4f08a841",
            "f0134525711646259f7546eb3baad313",
            "6d8fb5f55f674889aa1e4ce14403bf3b",
            "5f77157dfdb0432390e904843a43084b",
            "393b1570c55f401ea17e243833859be5",
            "e9241e0ea9ff4343ada4085a8efa7f41",
            "383a7c545c674930ac382065d7016934",
            "8387937c70154192a7125699c62d25dd",
            "36f6001890404d93a107c3644b4f06c8",
            "f5e3e2e91d9043d49511f322f2639894",
            "e2413b7c362c424db961f7edad2d8e8d",
            "34bbe378a1fa4b628d3314792ff5d0a6",
            "48052552fe2b43e791fbf90e59fb2f2f",
            "a10296da62e24a278043208bdf52a7ab",
            "1d9f282bd38d4ed5a45207f860e8b6a7",
            "6f2b184dd0cf4569bfc37f1e1afba72a",
            "11bf5755ed3a43249f0491284afb3972",
            "46a2e6262b214e9c82d4272c59a58425",
            "e22e8c249793416a881af549062333c1",
            "2653d81996e7449380a8c1c728668623",
            "16c49fe73d984f18a26f878c32eb35b0"
          ]
        },
        "id": "mZnCGnZfeLtT",
        "outputId": "376cd18c-154b-4875-bf28-4195e5d98368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0835d2bfe0514442a29d4fd72f86ece0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "564d51582a88411f9225fdba6c991cd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eef7b00c71e4a5cbfd58da987011cda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a6c8dc5c476458cbacf97c9d0b61745"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b64094b12ca4b6c8fbc579f09eefa55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5f80c69b25a4cb08af2f0498dbd5753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2413b7c362c424db961f7edad2d8e8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "image_classifier = pipeline(\n",
        "    task=\"image-classification\", model=\"google/vit-base-patch16-224\"\n",
        ")\n",
        "result = image_classifier(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "4495c57c8622442fb22b65161412e643",
            "c3b1649108754e1ca205dfeed9355e28",
            "995b3e73b76647f79d0573a35604e8db",
            "2b869c574f01416e80694a3f7dcf329d",
            "3c18ac26c95c4bf6a322ec7cca41836d",
            "a75b2bb5fda943c9a628ac4719185c2d",
            "8807ebc02c2f4b5780bcefb8b906243d",
            "b1c7d69980234ed390d008c922c1c78a",
            "bb0e8f3db169429fa62d3f4ccbd4f8de",
            "d17a4589bf1044afacb37927c42b0d0f",
            "3e758991362d47c694fcd38939aaa029",
            "72570b2efc2746a4a55d7138fa5d86ef",
            "524323a59f334be9965742e121480b31",
            "5cfc48c8b5a845f89d407077ae6452d0",
            "bc5b0876da804b9faa31fab809b652d7",
            "84739aa63dd6497088aac256bfc85c07",
            "4879425eda164b349d22dfb7ba60dec6",
            "f702f13d6e2b4529b3be7d0d53dda31b",
            "a1f33851864540d4921ea8f0f09c47c4",
            "9eb80f0735b4454faf5b3dfaac4f0025",
            "9bed9b3d896e4105a4bdad4136f7890f",
            "66dee8254dff466495c4ce0c1806696b",
            "2eeec6402d1943ceb2bde38252fe380c",
            "41fca396e30a4d7d9888954647d87453",
            "e81eec8742c5403fb540243f358235a5",
            "2bb200c772a6450ab5c51476eb04b231",
            "c08e421c6a4541d993aa31474a80546e",
            "8b925acf703948aba710260d42afe2e7",
            "63a037f80d2e45ccbec83021818c4964",
            "87c7fe33f6d249d694f27f38fd4fb867",
            "098e7e8037e5481ebb76e7d508e1320a",
            "eaee3fdce34a4e9fa7efbd52186cabf5",
            "6bae76d8af4247f99f611275396d7d83"
          ]
        },
        "id": "h5GMViayeWXB",
        "outputId": "d53fd89c-17ad-4ad7-d85a-ee8bf615fae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4495c57c8622442fb22b65161412e643"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72570b2efc2746a4a55d7138fa5d86ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eeec6402d1943ceb2bde38252fe380c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'lynx, catamount', 'score': 0.4334997534751892}, {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.03479616343975067}, {'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.032401930540800095}, {'label': 'Egyptian cat', 'score': 0.023944787681102753}, {'label': 'tiger cat', 'score': 0.02288924530148506}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "transcriber = pipeline(\n",
        "    task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\n",
        ")\n",
        "result = transcriber(\n",
        "    \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815,
          "referenced_widgets": [
            "0d3b8bc0fb0145bd97bed577d663cac4",
            "c337d746823346d0a7ceab2b2b74f135",
            "780070f924ab49d884beeeebc71aff0e",
            "1acc90a1f254404b902d3d71ab606453",
            "132d9c2213a54e838ba2a54a68c50e27",
            "f0826235ab42477796324094f63a2bf5",
            "2ed72df41ada4ef98e392bd90bdd0b0d",
            "9943ac17bde44cbbbf0f6c3e621cfbd7",
            "067877cf65ff4fdeb2002a3805af46ce",
            "adf65284b9fa49e4a0a0760861de6149",
            "b7e4247994e74aad9d4aad8efaf479a7",
            "4fcc2a543cb24cb3b82caaa847c3157b",
            "f0ec84ccb1414bbf965e0deac191d040",
            "ff38685f8a5e45bc82a652c554fb6b77",
            "c155b04d7ab542f6b4fa7d5634f08bac",
            "88186639f3dd4b86a92904b1d8737b99",
            "70fd4e0090474818a221b66284338cb2",
            "b6c11d2046a44b11bd04a634202d662d",
            "22ea2d40e0584c8db6cca07155364314",
            "1684343f1f0e4dea916c20a1dd6eef14",
            "3d48c8797b3542d9a10f4c643b28b303",
            "f09e24483f6a46c6b9bf72c3db2f2866",
            "6d892526cb1147eeb6a3865c34cf8968",
            "34d75a15abcf471aaa5a5f9133f4777a",
            "2e079346007b483da7865dc3791a3083",
            "624541e8d783423498b0c748bd11fed9",
            "2e04cad4cc6e444f81bd5be65f665872",
            "90cba2d63b594d139aac93b6623698a0",
            "bc5c4f004878461f9d3d41b27f8c8b6c",
            "35c59e1c0c54406582d374817e422901",
            "67f5bbd3edb345a1b371a108ebf61ade",
            "9c6a6e39d090407190e1024c18310e9e",
            "9a97e2cf2b8d4baaae9e412bbaeb27d8",
            "c92902beb5d44fc8b6c47f6cdbb3552f",
            "bd094449e3564c859425d86336b79f5f",
            "088ab1f0ecfb43fcbffddc2e18da5a41",
            "6ce1fae9c19f4170b0083c4f2630ad35",
            "73cbb12dbba84f489396401ee047bbe4",
            "eb6adb19188d4ab5b6dc94bc495508a7",
            "777ba80cb9c44461b4e7b2c4c5afbace",
            "de6c0826afe94fe5aa3209489c464846",
            "34318777d82f4ad5b41def0f9be2c8b1",
            "8a395970ed6c45fcb08f83ee9881f2df",
            "987ad7d25018408ba3bd5c255463dd82",
            "3ec879a260ba44aaad197b191c922b45",
            "6922efcacf684e04a6832a5b671c4240",
            "63207b8ffb884586823e09a4d0f31902",
            "e299e5712c2e4f7998c98ace39cc977e",
            "33b1dd394add4ec99925e33e95092dc0",
            "d45474afa97246c6ab2697b5121e40f1",
            "263cc4385ee24613b0aabe90e2236349",
            "cf3691dc05cb4eabbb566fa4b7e01f04",
            "992d9b9dd77e40d3b3e6b696e5d7e4ba",
            "7da4945a69254402b44f17da2a2af528",
            "f5d6210fef574883b06965a7d253e70b",
            "45b2715ede1e4eebad4f9f5bb5b384ef",
            "0900870d8ef04670b621bdca70643513",
            "8f050cd9f9d343c88f3e7e1b103af1b6",
            "00fb65527812452e9d824f477ab0b6ea",
            "539e626a3bdc4bcb80e08ad58149b269",
            "f2fd064495ce4336b52cac1b50d50bdf",
            "53d5ebe4821f4d48bd8f7b0a5b0d0418",
            "5ebfb61779594f1797f7fc8d3c9327d0",
            "c71820cd085e4359912cd7b98eb87524",
            "5cf7b4bc748240049da7a4649fa43792",
            "dfa6f92fa7b14fee8d4d88879c88c98d",
            "d8f38a173541404ea685efe14808c637",
            "8717a536e7f64ef6869c5f8c0ff6b08b",
            "f145bcafbfb24b0d93d4ee47aa919c45",
            "c3d8019334e44d0b868f161818eee416",
            "76f6fefe390e410abbe04121e716b0e2",
            "c5541ad9d8ea4cf3bbb1f2664634b71c",
            "235048213e474eb3aa75da15d0d0d0ef",
            "17234360b22d462e82dc87ff64e8697f",
            "7272b48c172c4bc7b064f2d778d04aa6",
            "bfaa0b6a8cf04d42aa45e0ade3a3fdbf",
            "6af23e551e304679b22ed9a6f54cbeb4",
            "88edba88dddc4b848070e30ab03d6cb7",
            "404a70e84e7e4a278fbcc75b19ff13ec",
            "5cc8994c5002412b9ab1ae0bb12758c4",
            "41711246c3224d53bdd059d0c1840d91",
            "e9eb9bb5582241e88a6b3e8d9cf0c257",
            "bdee73eaba58462f91993aa1d4cb4ae0",
            "0d4aa1185c4d445095e4529fe72527bd",
            "5d7dd1dca8bf4df38cfd1c9ec531976a",
            "d89e6d2a84994618bcb726c49f8a65a6",
            "c4ade99c87d44e628f79c3267c3ae484",
            "0b932bee4e8b43f28821bd2b7d36cdb1",
            "3780af82073f45b5a71490f3252750ee",
            "23cd20c720494d73b44c01c514060fc7",
            "39b1734e03a9404189745038b95bef2f",
            "79d256e1ca2c4b6093b1c7f5f32e2a7c",
            "debf44e24b044fd997332bf042cd980b",
            "8061dcebc8bb4c6e983659c890f7465d",
            "d265184a1751442699d3851e63930977",
            "d703b3f04a874b429f292170d6c2bdd5",
            "60026c870b384fe5b11ad85cd86fe064",
            "c6646d4475674d4787e50ec19f48597a",
            "2a6d2ea89df14c049542af2c0abb131a",
            "502b7495abec434e9e437fa3df40b911",
            "f88df8b1a7dc4cbdba9b5720a759387e",
            "8d1688a81d8f4b4abee2f865556c4d8f",
            "0af7bba3858f4881a218a50618983c3b",
            "ec61b63399a14a57bdd244437a61162c",
            "69a30aac75b44e85823107e8606f0cb6",
            "76b443f94ae44df5885e61d333d46a3a",
            "5ea8b42c2750408280f87df9a5b46362",
            "28959e9eb07a41d79b2c1d7bd0335fd1",
            "05e41e91655341f88bcc5d0b244cdfc1",
            "7296605c01f84143be3c56969e4b47fe",
            "5c2a5d4c54c34dd584a741e0fd4105e3",
            "120c0e9af70d4c0aa1975cb510f01841",
            "2b7ddd10f8384fe598452f6b434800bf",
            "40cc09551c4846368d6c8b519b828c2f",
            "e8fa3077f6024c32bca832dc5b897797",
            "c8486f64749f4697bede4df993ac42fb",
            "e6c26830db8246409ce268a97ad35080",
            "6ba09ed6b9364713bfd675e0d4efaf0a",
            "c33d4566805642eba5adb499dbeb952e",
            "3f74db0199594d8eb8a74b6649395757",
            "12cb584603b64793a296d8eb9bfd75a4"
          ]
        },
        "id": "PxsZatakenG3",
        "outputId": "c5cf841c-11c0-4df7-ee5b-56d23d3d3a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d3b8bc0fb0145bd97bed577d663cac4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fcc2a543cb24cb3b82caaa847c3157b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d892526cb1147eeb6a3865c34cf8968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c92902beb5d44fc8b6c47f6cdbb3552f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec879a260ba44aaad197b191c922b45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45b2715ede1e4eebad4f9f5bb5b384ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8f38a173541404ea685efe14808c637"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88edba88dddc4b848070e30ab03d6cb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3780af82073f45b5a71490f3252750ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "502b7495abec434e9e437fa3df40b911"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c2a5d4c54c34dd584a741e0fd4105e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-190009159.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"automatic-speech-recognition\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai/whisper-large-v3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0;31m result = transcriber(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/automatic_speech_recognition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0;31m`\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chunks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     def _sanitize_parameters(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1460\u001b[0m                 iter(\n\u001b[1;32m   1461\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/automatic_speech_recognition.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             }\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;31m# whisper longform generation stores timestamps in \"segments\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/generation_whisper.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0mdo_condition_on_prev_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mmodel_output_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0msegment_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m                 \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/generation_whisper.py\u001b[0m in \u001b[0;36mgenerate_with_fallback\u001b[0;34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     )\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             seek_outputs = super().generate(\n\u001b[0m\u001b[1;32m   1039\u001b[0m                 \u001b[0msegment_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_running_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3265\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3267\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1287\u001b[0m                 )\n\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             hidden_states, cross_attn_weights = self.encoder_attn(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}